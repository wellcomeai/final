<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Голосовой Ассистент</title>
    <style>
        body { 
            font-family: Arial, sans-serif; 
            max-width: 800px; 
            margin: 0 auto; 
            padding: 20px;
            background-color: #f5f5f5;
        }
        h1 {
            text-align: center;
            color: #333;
        }
        #micStatus { 
            width: 150px; height: 150px; 
            border-radius: 75px; 
            background-color: #4CAF50; 
            margin: 0 auto;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 18px;
            text-align: center;
            transition: transform 0.1s, background-color 0.3s;
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
            cursor: pointer;
        }
        #micStatus.listening { 
            background-color: #FF5722;
            animation: pulse 1.5s infinite;
        }
        #micStatus.assistant-speaking { 
            background-color: #2196F3;
            animation: pulse 1s infinite;
        }
        #micStatus.disabled { background-color: #ccc; }
        
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }
        
        #status, #transcript, #debug { 
            margin: 20px 0; 
            padding: 15px; 
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        #status {
            background-color: #e3f2fd;
            font-weight: bold;
            text-align: center;
        }
        #transcript {
            background-color: white;
            max-height: 300px;
            overflow-y: auto;
            line-height: 1.5;
        }
        #debug {
            background-color: #f0f0f0;
            max-height: 200px;
            overflow-y: auto;
            font-family: monospace;
            font-size: 12px;
        }
        .message {
            margin-bottom: 15px;
            padding-bottom: 15px;
            border-bottom: 1px solid #eee;
        }
        .user {
            color: #4CAF50;
            font-weight: bold;
        }
        .assistant {
            color: #2196F3;
            font-weight: bold;
        }
        .debug-toggle {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 10px 0;
        }
        .instructions {
            background: #e7f3ff;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            text-align: center;
        }
    </style>
</head>
<body>
    <h1>Голосовой Ассистент</h1>
    
    <div id="micStatus">Подключение...</div>
    
    <div id="status">Инициализация...</div>
    
    <div class="instructions">
        <strong>Просто говорите!</strong> Ассистент автоматически услышит ваш голос и ответит.
    </div>
    
    <div id="transcript">История диалога появится здесь</div>
    
    <button class="debug-toggle" onclick="toggleDebug()">Показать/скрыть отладку</button>
    <div id="debug" style="display: none;">Логи отладки появятся здесь</div>
    
    <script>
        let socket;
        let audioContext;
        let audioProcessor;
        let mediaStream;
        let isConnected = false;
        let isListening = false;
        let isAssistantSpeaking = false;
        let assistantAudio = null;
        
        // Настройки для Voice Activity Detection
        const VAD_CONFIG = {
            silenceThreshold: 0.01,      // Порог тишины
            soundThreshold: 0.02,        // Порог звука  
            silenceDuration: 1000,       // Длительность тишины для отправки (мс)
            minAudioLength: 500,         // Минимальная длительность аудио (мс)
            bufferCheckInterval: 50      // Частота проверки буфера (мс)
        };
        
        let audioBuffer = [];
        let isSilent = true;
        let silenceStartTime = Date.now();
        let audioStartTime = 0;
        let hasAudioData = false;
        
        // Элементы страницы
        const micStatusDiv = document.getElementById('micStatus');
        const statusDiv = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');
        const debugDiv = document.getElementById('debug');
        
        // Функция отладки
        function log(message, level = 'INFO') {
            const timestamp = new Date().toLocaleTimeString();
            const logMessage = `[${timestamp}] [${level}] ${message}`;
            console.log(logMessage);
            
            debugDiv.innerHTML += logMessage + '\n';
            debugDiv.scrollTop = debugDiv.scrollHeight;
        }
        
        function toggleDebug() {
            debugDiv.style.display = debugDiv.style.display === 'none' ? 'block' : 'none';
        }
        
        // Автоматически подключаемся при загрузке страницы
        window.addEventListener('load', init);
        
        async function init() {
            log('Инициализация приложения');
            await setupAudioProcessing();
            connectWebSocket();
        }
        
        // Настройка аудио обработки с правильным форматом PCM16
        async function setupAudioProcessing() {
            try {
                log('Запрос доступа к микрофону');
                
                // Запрашиваем доступ к микрофону с нужными параметрами
                mediaStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: { 
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 24000,
                        channelCount: 1
                    } 
                });
                
                // Создаем AudioContext с нужной частотой дискретизации
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 24000
                });
                
                log(`AudioContext создан с частотой ${audioContext.sampleRate} Hz`);
                
                // Создаем источник аудио
                const audioSource = audioContext.createMediaStreamSource(mediaStream);
                
                // Создаем ScriptProcessorNode для обработки аудио
                const bufferSize = 2048; // Размер буфера
                audioProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1);
                
                // Обработчик аудио данных
                audioProcessor.onaudioprocess = function(event) {
                    if (!isListening || isAssistantSpeaking) return;
                    
                    const inputData = event.inputBuffer.getChannelData(0); // Моно канал
                    
                    // Вычисляем уровень громкости
                    let sum = 0;
                    for (let i = 0; i < inputData.length; i++) {
                        sum += Math.abs(inputData[i]);
                    }
                    const averageLevel = sum / inputData.length;
                    
                    // Определяем, есть ли звук
                    const hasSound = averageLevel > VAD_CONFIG.soundThreshold;
                    
                    // Конвертируем Float32 в Int16 PCM
                    const pcm16Data = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        // Ограничиваем значения и конвертируем в Int16
                        const sample = Math.max(-1, Math.min(1, inputData[i]));
                        pcm16Data[i] = Math.floor(sample * 32767);
                    }
                    
                    // Добавляем данные в буфер
                    audioBuffer.push(new Uint8Array(pcm16Data.buffer));
                    
                    // Отправляем аудио чанк сразу
                    if (socket && socket.readyState === WebSocket.OPEN && hasSound) {
                        const base64Audio = arrayBufferToBase64(pcm16Data.buffer);
                        socket.send(JSON.stringify({
                            type: "input_audio_buffer.append",
                            audio: base64Audio,
                            event_id: `audio_${Date.now()}`
                        }));
                    }
                    
                    // Voice Activity Detection
                    const now = Date.now();
                    
                    if (hasSound) {
                        if (isSilent) {
                            log('Обнаружен голос, начинаем запись');
                            isSilent = false;
                            audioStartTime = now;
                            hasAudioData = true;
                            micStatusDiv.classList.add('listening');
                            micStatusDiv.textContent = 'Слушаю...';
                            statusDiv.textContent = 'Слушаю вас...';
                        }
                        silenceStartTime = now; // Сбрасываем время начала тишины
                    } else if (!isSilent) {
                        // Проверяем, достаточно ли длилась тишина
                        const silenceDuration = now - silenceStartTime;
                        
                        if (silenceDuration > VAD_CONFIG.silenceDuration) {
                            const audioLength = now - audioStartTime;
                            
                            // Проверяем минимальную длительность аудио
                            if (audioLength > VAD_CONFIG.minAudioLength && hasAudioData) {
                                log(`Тишина ${silenceDuration}мс, отправляем аудио (${audioLength}мс)`);
                                commitAudio();
                            } else {
                                log(`Аудио слишком короткое (${audioLength}мс), сбрасываем буфер`);
                                clearAudioBuffer();
                            }
                            
                            isSilent = true;
                            hasAudioData = false;
                            micStatusDiv.classList.remove('listening');
                            micStatusDiv.textContent = 'Говорите';
                            statusDiv.textContent = 'Готов к общению!';
                        }
                    }
                };
                
                // Подключаем обработчик
                audioSource.connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);
                
                log('Аудио обработка настроена успешно');
                statusDiv.textContent = 'Микрофон подключен';
                
            } catch (error) {
                log(`Ошибка настройки аудио: ${error}`, 'ERROR');
                statusDiv.textContent = 'Ошибка доступа к микрофону';
                micStatusDiv.textContent = 'Ошибка';
                micStatusDiv.classList.add('disabled');
            }
        }
        
        // Функция подключения к WebSocket
        function connectWebSocket() {
            const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${wsProtocol}//${window.location.host}/ws/demo`;
            
            log(`Подключение к WebSocket: ${wsUrl}`);
            
            socket = new WebSocket(wsUrl);
            
            socket.onopen = function() {
                log('WebSocket соединение открыто');
                statusDiv.textContent = 'Соединение установлено';
                isConnected = true;
                micStatusDiv.textContent = 'Подготовка...';
            };
            
            socket.onmessage = function(event) {
                if (event.data instanceof Blob) {
                    log('Получено аудио от ассистента');
                    playAssistantAudio(event.data);
                } else if (event.data instanceof ArrayBuffer) {
                    log('Получен аудио буфер от ассистента');
                    playAssistantAudioBuffer(event.data);
                } else {
                    try {
                        const data = JSON.parse(event.data);
                        log(`Получено сообщение: ${data.type}`);
                        
                        // Обработка статуса соединения
                        if (data.type === 'connection_status' && data.status === 'connected') {
                            statusDiv.textContent = 'Готов к общению!';
                            micStatusDiv.textContent = 'Говорите';
                            log('Соединение с ассистентом установлено');
                            
                            // Автоматически начинаем слушать
                            startListening();
                        }
                        
                        // Обработка начала ответа ассистента
                        if (data.type === 'assistant_thinking') {
                            log('Ассистент начинает думать');
                            micStatusDiv.textContent = 'Думаю...';
                            statusDiv.textContent = 'Ассистент обдумывает ответ...';
                        }
                        
                        // Обработка текстового ответа (если нет аудио)
                        if (data.type === 'text_response') {
                            log(`Получен текстовый ответ: ${data.text}`);
                            addToTranscript('assistant', data.text);
                            
                            // Показываем, что ответ получен
                            micStatusDiv.textContent = 'Говорите';
                            statusDiv.textContent = 'Готов к общению!';
                        }
                        
                        // Обработка завершения ответа
                        if (data.type === 'assistant_response_complete') {
                            log('Ответ ассистента завершен');
                        }
                        
                        // Обработка транскрипций
                        if (data.type === 'conversation.item.input_audio_transcription.completed' && data.transcript) {
                            addToTranscript('user', data.transcript);
                            log(`Транскрипция пользователя: ${data.transcript}`);
                        } else if (data.type === 'response.audio_transcript.done' && data.transcript) {
                            addToTranscript('assistant', data.transcript);
                            log(`Транскрипция ассистента: ${data.transcript}`);
                        }
                        
                        // Обработка создания ответа
                        if (data.type === 'response.created') {
                            log('Ассистент начал генерировать ответ');
                            micStatusDiv.textContent = 'Генерирую ответ...';
                            statusDiv.textContent = 'Ассистент генерирует ответ...';
                        }
                        
                        // Обработка аудио ответа
                        if (data.type === 'response.audio.delta') {
                            log('Получен фрагмент аудио ответа');
                            // Аудио будет обработано как бинарные данные
                        }
                        
                        // Обработка завершения аудио
                        if (data.type === 'response.audio.done') {
                            log('Аудио ответ завершен');
                        }
                        
                        // Обработка ошибок
                        if (data.type === 'error') {
                            statusDiv.textContent = `Ошибка: ${data.error.message}`;
                            log(`Ошибка: ${data.error.message}`, 'ERROR');
                        }
                        
                        // Обработка подтверждений
                        if (data.type === 'input_audio_buffer.commit.ack') {
                            log('Аудио буфер успешно отправлен');
                            micStatusDiv.textContent = 'Обрабатываю...';
                            statusDiv.textContent = 'Обрабатываю запрос...';
                        }
                        
                        // Обработка Speech Started (важно для Server VAD)
                        if (data.type === 'input_audio_buffer.speech_started') {
                            log('Сервер обнаружил начало речи');
                        }
                        
                        // Обработка Speech Stopped
                        if (data.type === 'input_audio_buffer.speech_stopped') {
                            log('Сервер обнаружил конец речи');
                        }
                    } catch (e) {
                        log(`Ошибка парсинга сообщения: ${e}`, 'ERROR');
                    }
                }
            };
            
            socket.onclose = function(event) {
                log(`WebSocket соединение закрыто: код ${event.code}`, 'WARN');
                statusDiv.textContent = 'Соединение закрыто. Перезагрузите страницу.';
                isConnected = false;
                micStatusDiv.textContent = 'Отключено';
                micStatusDiv.classList.add('disabled');
                stopListening();
            };
            
            socket.onerror = function(error) {
                log(`WebSocket ошибка: ${error}`, 'ERROR');
                statusDiv.textContent = 'Ошибка соединения';
            };
        }
        
        // Функция начала прослушивания
        function startListening() {
            if (!isConnected || !audioContext || isListening) {
                log('Нельзя начать прослушивание', 'WARN');
                return;
            }
            
            log('Начинаем прослушивание');
            isListening = true;
            
            // Возобновляем AudioContext если нужно
            if (audioContext.state === 'suspended') {
                audioContext.resume();
            }
            
            micStatusDiv.textContent = 'Говорите';
            statusDiv.textContent = 'Готов к общению!';
            
            // Очищаем буфер при начале нового сеанса
            clearAudioBuffer();
        }
        
        // Функция остановки прослушивания
        function stopListening() {
            log('Остановка прослушивания');
            isListening = false;
            clearAudioBuffer();
        }
        
        // Функция отправки аудио буфера
        function commitAudio() {
            if (!socket || socket.readyState !== WebSocket.OPEN || !hasAudioData) {
                log('Нельзя отправить аудио: нет соединения или данных', 'WARN');
                return;
            }
            
            log(`Отправка аудио буфера, чанков: ${audioBuffer.length}`);
            
            // Отправляем команду завершения ввода
            socket.send(JSON.stringify({
                type: "input_audio_buffer.commit",
                event_id: `commit_${Date.now()}`
            }));
            
            // Очищаем буфер
            audioBuffer = [];
            hasAudioData = false;
        }
        
        // Функция очистки аудио буфера
        function clearAudioBuffer() {
            audioBuffer = [];
            hasAudioData = false;
            
            if (socket && socket.readyState === WebSocket.OPEN) {
                socket.send(JSON.stringify({
                    type: "input_audio_buffer.clear",
                    event_id: `clear_${Date.now()}`
                }));
            }
        }
        
        // Добавление сообщений в историю диалога
        function addToTranscript(role, text) {
            const messageDiv = document.createElement('div');
            messageDiv.className = 'message';
            
            const roleSpan = document.createElement('span');
            roleSpan.className = role;
            roleSpan.textContent = role === 'user' ? 'Вы: ' : 'Ассистент: ';
            
            messageDiv.appendChild(roleSpan);
            messageDiv.appendChild(document.createTextNode(text));
            
            // Вставляем новое сообщение в начало
            if (transcriptDiv.firstChild) {
                transcriptDiv.insertBefore(messageDiv, transcriptDiv.firstChild);
            } else {
                transcriptDiv.appendChild(messageDiv);
            }
        }
        
        // Функция воспроизведения аудио от ассистента (Blob)
        function playAssistantAudio(audioBlob) {
            log('Воспроизведение аудио ассистента (Blob)');
            
            // Останавливаем прослушивание пока говорит ассистент
            isAssistantSpeaking = true;
            micStatusDiv.classList.add('assistant-speaking');
            micStatusDiv.classList.remove('listening');
            micStatusDiv.textContent = 'Ассистент говорит';
            statusDiv.textContent = 'Ассистент отвечает...';
            
            const audioUrl = URL.createObjectURL(audioBlob);
            assistantAudio = new Audio(audioUrl);
            
            assistantAudio.onended = function() {
                log('Воспроизведение завершено');
                URL.revokeObjectURL(audioUrl);
                isAssistantSpeaking = false;
                micStatusDiv.classList.remove('assistant-speaking');
                micStatusDiv.textContent = 'Говорите';
                statusDiv.textContent = 'Готов к общению!';
                
                // Возобновляем прослушивание
                if (isListening) {
                    clearAudioBuffer(); // Очищаем буфер перед новым циклом
                }
            };
            
            assistantAudio.play().catch(error => {
                log(`Ошибка воспроизведения: ${error}`, 'ERROR');
                isAssistantSpeaking = false;
                micStatusDiv.classList.remove('assistant-speaking');
            });
        }
        
        // Функция воспроизведения аудио от ассистента (ArrayBuffer)
        function playAssistantAudioBuffer(arrayBuffer) {
            log('Воспроизведение аудио ассистента (ArrayBuffer)');
            
            // Конвертируем ArrayBuffer в Blob
            const audioBlob = new Blob([arrayBuffer], { type: 'audio/wav' });
            playAssistantAudio(audioBlob);
        }
        
        // Конвертация ArrayBuffer в Base64
        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }
        
        // Обработчик клика на микрофон (для отладки)
        micStatusDiv.addEventListener('click', function() {
            if (isListening) {
                log('Принудительная отправка аудио по клику');
                commitAudio();
            }
        });
    </script>
</body>
</html>
